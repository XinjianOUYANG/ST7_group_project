{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "black-blogger",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-standard",
   "metadata": {},
   "source": [
    "## Import dependies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hidden-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.4.1\n",
      "keras 2.4.0\n",
      "Importing successfully!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD # Stochastic gradient descent\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap # seaborn: statistical data visualization\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('tensorflow',tf.__version__)\n",
    "print('keras',keras.__version__)\n",
    "print('Importing successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-alexander",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ultimate-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 128 # batch size\n",
    "\n",
    "def get_datagen(dataset, aug=False):\n",
    "    if aug:\n",
    "        datagen = ImageDataGenerator(\n",
    "                            rescale=1./255,\n",
    "                            featurewise_center=False,\n",
    "                            featurewise_std_normalization=False,\n",
    "                            rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            horizontal_flip=True)\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    return datagen.flow_from_directory(\n",
    "            dataset,\n",
    "            target_size =(48, 48),\n",
    "            color_mode ='grayscale',\n",
    "            shuffle = True,\n",
    "            class_mode = 'categorical',\n",
    "            batch_size = BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "authorized-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator  = get_datagen('../FER_Dataset/train', True)\n",
    "dev_generator    = get_datagen('../FER_Dataset/test-private')\n",
    "test_generator  = get_datagen('../FER_Dataset/test-public')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-lafayette",
   "metadata": {},
   "source": [
    "## Build the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "involved-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout_rate = 0.3\n",
    "SGD_lr = 0.01 # learning rate of SGD optimiser\n",
    "SGD_decay = 0.0001 #decay of SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "resident-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(BatchNormalization(input_shape=(48,48,1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same',name=\"conv2\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name=\"conv3\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name=\"conv4\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool4\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu',name='fc1'))\n",
    "model.add(Dropout(Dropout_rate))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
    "\n",
    "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
    "sgd = SGD(lr=SGD_lr, momentum=0.9, decay=SGD_decay, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "# checkpoint\n",
    "cp_filepath='models/Baseline-weights-best.hdf5'\n",
    "rlrop = keras.callbacks.ReduceLROnPlateau(monitor='val_acc',mode='max',factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
    "checkpoint = ModelCheckpoint(cp_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [rlrop,checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-salad",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ouyang/opt/anaconda3/envs/AudioImage/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "225/225 [==============================] - 126s 556ms/step - loss: 2.1824 - accuracy: 0.2282 - val_loss: 2.1139 - val_accuracy: 0.2388\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/2\n",
      "194/225 [========================>.....] - ETA: 17s - loss: 1.8046 - accuracy: 0.2940"
     ]
    }
   ],
   "source": [
    "Epochs = 2\n",
    "history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    validation_data=dev_generator, \n",
    "    #steps_per_epoch=28709// BS,\n",
    "    #validation_steps=3509 // BS,\n",
    "    shuffle=True,\n",
    "    epochs=Epochs,\n",
    "    callbacks=callbacks_list,\n",
    "#    callbacks=[rlrop],\n",
    "    use_multiprocessing=False,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on dev data')\n",
    "results_dev = model.evaluate_generator(dev_generator, 3509 // BS)\n",
    "print('dev loss, dev acc:', results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results_test = model.evaluate_generator(test_generator, 3509 // BS)\n",
    "print('test loss, test acc:', results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_str = '-EPOCHS_' + str(EPOCHS)\n",
    "dropout_str = '-DROPOUT_' + str(DROPOUT_RATE)\n",
    "test_acc = '-test_acc_%.3f' % results_test[1]\n",
    "model.save('models/' + 'SOA' + epoch_str + dropout_str + test_acc + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}\n",
    "\n",
    "y_pred = model.predict_generator(dev_generator).argmax(axis=1)\n",
    "y_true = dev_generator.classes\n",
    "\n",
    "cmat_df_test=pd.DataFrame(\n",
    "  confusion_matrix(y_true, y_pred, normalize='true').round(2),\n",
    "  index=emotions.values(), \n",
    "  columns=emotions.values()\n",
    "  )\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "heatmap(cmat_df_test,annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion Matrix on Private Test Set')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# configure image data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "# make a prediction using test-time augmentation\n",
    "def tta_prediction(datagen, model, image, n_examples):\n",
    "\t# convert image into dataset\n",
    "\tsamples = np.expand_dims(image, 0)\n",
    "\t# prepare iterator\n",
    "\tit = datagen.flow(samples, batch_size=n_examples)\n",
    "\t# make predictions for each augmented image\n",
    "\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
    "\t# sum across predictions\n",
    "\tsummed = np.sum(yhats, axis=0)\n",
    "\t# argmax across classes\n",
    "\treturn np.argmax(summed)\n",
    " \n",
    " # evaluate a model on a dataset using test-time augmentation\n",
    "def tta_evaluate_model(model, testX, testY):\n",
    "\t# configure image data augmentation\n",
    "\tdatagen = ImageDataGenerator(horizontal_flip=True)\n",
    "\t# define the number of augmented images to generate per test set image\n",
    "\tn_examples_per_image = 7\n",
    "\tyhats = list()\n",
    "\tfor i in range(len(testX)):\n",
    "\t\t# make augmented prediction\n",
    "\t\tyhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n",
    "\t\t# store for evaluation\n",
    "\t\tyhats.append(yhat)\n",
    "\t# calculate accuracy\n",
    "\ttestY_labels = np.argmax(testY, axis=1)\n",
    "\tacc = accuracy_score(testY_labels, yhats)\n",
    "\treturn acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "#TTA_results_test = tta_evaluate_model(model, X_test, Y_test)\n",
    "print('test loss, test acc:', results_test)\n",
    "print('TTA test acc:', TTA_results_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
