{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "black-blogger",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-standard",
   "metadata": {},
   "source": [
    "## Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hidden-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.4.1\n",
      "keras 2.4.0\n",
      "Importing successfully!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD # Stochastic gradient descent\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap # seaborn: statistical data visualization\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Importing successfully!')\n",
    "print('tensorflow',tf.__version__)\n",
    "print('keras',keras.__version__)\n",
    "print('GPU',tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-alexander",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ultimate-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 128 # batch size\n",
    "\n",
    "def get_datagen(dataset, aug=False):\n",
    "    if aug:\n",
    "        datagen = ImageDataGenerator(\n",
    "                            rescale=1./255,\n",
    "                            featurewise_center=False,\n",
    "                            featurewise_std_normalization=False,\n",
    "                            rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            horizontal_flip=True)\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    return datagen.flow_from_directory(\n",
    "            dataset,\n",
    "            target_size =(48, 48),\n",
    "            color_mode ='grayscale',\n",
    "            shuffle = True,\n",
    "            class_mode = 'categorical',\n",
    "            batch_size = BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "authorized-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator  = get_datagen(\"/Users/ouyang/Documents/GitHub/ST7_FER_Projet/FER_Dataset/train\", True)\n",
    "dev_generator    = get_datagen(\"/Users/ouyang/Documents/GitHub/ST7_FER_Projet/FER_Dataset/test-private\")\n",
    "test_generator  = get_datagen(\"/Users/ouyang/Documents/GitHub/ST7_FER_Projet/FER_Dataset/test-public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-lafayette",
   "metadata": {},
   "source": [
    "## Build the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "involved-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout_rate = 0.3\n",
    "SGD_lr = 0.01 # learning rate of SGD optimiser\n",
    "SGD_decay = 0.0001 #decay of SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "resident-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(48,48,1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same',name=\"conv2\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name=\"conv3\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name=\"conv4\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool4\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu',name='fc1'))\n",
    "model.add(Dropout(Dropout_rate))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
    "\n",
    "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
    "sgd = SGD(lr=SGD_lr, momentum=0.9, decay=SGD_decay, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-salad",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "104/225 [============>.................] - ETA: 1:04 - loss: 1.7528 - accuracy: 0.3235"
     ]
    }
   ],
   "source": [
    "Epochs = 2\n",
    "rlrop = keras.callbacks.ReduceLROnPlateau(monitor='val_acc',mode='max',factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
    "# checkpoint\n",
    "cp_filepath='ST7_models/Baseline-weights-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(cp_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    validation_data=dev_generator, \n",
    "    #steps_per_epoch=28709// BS,\n",
    "    #validation_steps=3509 // BS,\n",
    "    shuffle=True,\n",
    "    epochs=Epochs,\n",
    "    callbacks=[rlrop,checkpoint],\n",
    "#    callbacks=[rlrop],\n",
    "    use_multiprocessing=False,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-break",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-lending",
   "metadata": {},
   "source": [
    "    evaluate_generator(\n",
    "        generator, steps=None, callbacks=None, max_queue_size=10, workers=1,\n",
    "        use_multiprocessing=False, verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "traditional-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on dev data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ouyang/opt/anaconda3/envs/AudioImage/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev loss, dev acc: [1.6390408277511597, 0.35445600748062134]\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on dev data')\n",
    "results_dev = model.evaluate_generator(dev_generator, 3509 // BS)\n",
    "print('dev loss, dev acc:', results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "resistant-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "test loss, test acc: [1.647364854812622, 0.3559027910232544]\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results_test = model.evaluate_generator(test_generator, 3509 // BS)\n",
    "print('test loss, test acc:', results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "traditional-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4ce1a53d18a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_str = '-EPOCHS_' + str(EPOCHS)\n",
    "dropout_str = '-DROPOUT_' + str(DROPOUT_RATE)\n",
    "test_acc = '-test_acc_%.3f' % results_test[1]\n",
    "model.save('models/' + 'SOA' + epoch_str + dropout_str + test_acc + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}\n",
    "\n",
    "y_pred = model.predict_generator(dev_generator).argmax(axis=1)\n",
    "y_true = dev_generator.classes\n",
    "\n",
    "cmat_df_test=pd.DataFrame(\n",
    "  confusion_matrix(y_true, y_pred, normalize='true').round(2),\n",
    "  index=emotions.values(), \n",
    "  columns=emotions.values()\n",
    "  )\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "heatmap(cmat_df_test,annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion Matrix on Private Test Set')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# configure image data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "# make a prediction using test-time augmentation\n",
    "def tta_prediction(datagen, model, image, n_examples):\n",
    "    # convert image into dataset\n",
    "    samples = np.expand_dims(image, 0)\n",
    "    # prepare iterator\n",
    "    it = datagen.flow(samples, batch_size=n_examples)\n",
    "    # make predictions for each augmented image\n",
    "    yhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
    "    # sum across predictions\n",
    "    summed = np.sum(yhats, axis=0)\n",
    "    # argmax across classes\n",
    "    return np.argmax(summed)\n",
    " \n",
    " # evaluate a model on a dataset using test-time augmentation\n",
    "def tta_evaluate_model(model, testX, testY):\n",
    "    # configure image data augmentation\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "    # define the number of augmented images to generate per test set image\n",
    "    n_examples_per_image = 7\n",
    "    yhats = list()\n",
    "    for i in range(len(testX)):\n",
    "        # make augmented prediction\n",
    "        yhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n",
    "        # store for evaluation\n",
    "        yhats.append(yhat)\n",
    "    # calculate accuracy\n",
    "    testY_labels = np.argmax(testY, axis=1)\n",
    "    acc = accuracy_score(testY_labels, yhats)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "#TTA_results_test = tta_evaluate_model(model, X_test, Y_test)\n",
    "print('test loss, test acc:', results_test)\n",
    "print('TTA test acc:', TTA_results_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
