{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "black-blogger",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-standard",
   "metadata": {},
   "source": [
    "## Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD # Stochastic gradient descent\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap # seaborn: statistical data visualization\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Importing successfully!')\n",
    "print('tensorflow',tf.__version__)\n",
    "print('keras',keras.__version__)\n",
    "print('GPU',tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-alexander",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 128 # batch size\n",
    "\n",
    "def get_datagen(dataset, aug=False):\n",
    "    if aug:\n",
    "        datagen = ImageDataGenerator(\n",
    "                            rescale=1./255,\n",
    "                            featurewise_center=False,\n",
    "                            featurewise_std_normalization=False,\n",
    "                            rotation_range=10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            horizontal_flip=True)\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    return datagen.flow_from_directory(\n",
    "            dataset,\n",
    "            target_size =(48, 48),\n",
    "            color_mode ='grayscale',\n",
    "            shuffle = True,\n",
    "            class_mode = 'categorical',\n",
    "            batch_size = BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator  = get_datagen(\"/Users/ouyang/Documents/GitHub/ST7_FER_Projet/FER_Dataset/train\", True)\n",
    "dev_generator    = get_datagen(\"/Users/ouyang/Documents/GitHub/ST7_FER_Projet/FER_Dataset/test-private\")\n",
    "test_generator  = get_datagen(\"/Users/ouyang/Documents/GitHub/ST7_FER_Projet/FER_Dataset/test-public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-lafayette",
   "metadata": {},
   "source": [
    "## Build the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout_rate = 0.3\n",
    "SGD_lr = 0.01 # learning rate of SGD optimiser\n",
    "SGD_decay = 0.0001 #decay of SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(48,48,1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same', input_shape=(48,48,1),name=\"conv1\"))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool1\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same',name=\"conv2\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool2\"))         \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name=\"conv3\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool3\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name=\"conv4\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"maxpool4\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu',name='fc1'))\n",
    "model.add(Dropout(Dropout_rate))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(7, activation='softmax',name='fcsoftmax'))\n",
    "\n",
    "#TODO: weight decay of 0.0001...initial learning rate is set to 0.01 and reduced by a factor of 2 at every 25 epoch\n",
    "sgd = SGD(lr=SGD_lr, momentum=0.9, decay=SGD_decay, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-salad",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-salmon",
   "metadata": {},
   "source": [
    "More information about **keras.model**, please refer to the [official tutorial](https://www.tensorflow.org/api_docs/python/tf/keras/Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 10\n",
    "\n",
    "# reduce learning rate while training\n",
    "rlrop = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',factor=0.5, patience=10, min_lr=0.00001,mode='max')\n",
    "# define the checkpoint\n",
    "cp_filepath='/Baseline-weights-best.hdf5'\n",
    "checkpoint = ModelCheckpoint(cp_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "hist = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    validation_data=dev_generator, \n",
    "    #steps_per_epoch=28709// BS,\n",
    "    #validation_steps=3509 // BS,\n",
    "    shuffle=True,\n",
    "    epochs=Epochs,\n",
    "    callbacks=[rlrop,checkpoint],\n",
    "#    callbacks=[rlrop],\n",
    "    use_multiprocessing=False,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the keys of the trained model:','\\n', hist.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-basin",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-accountability",
   "metadata": {},
   "source": [
    "    evaluate_generator(\n",
    "        generator, steps=None, callbacks=None, max_queue_size=10, workers=1,\n",
    "        use_multiprocessing=False, verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on dev data')\n",
    "results_dev = model.evaluate_generator(dev_generator, 3509 // BS)\n",
    "print('dev loss, dev acc:', results_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results_test = model.evaluate_generator(test_generator, 3509 // BS)\n",
    "print('test loss, test acc:', results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-spare",
   "metadata": {},
   "source": [
    "### Save the model in folder *trained_models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_str = '-EPOCHS_' + str(Epochs)\n",
    "dropout_str = '-DROPOUT_' + str(Dropout_rate)\n",
    "test_acc = '-test_acc_%.3f' % results_test[1]\n",
    "model.save('trained_models/' + 'Baseline' + epoch_str + dropout_str + test_acc + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}\n",
    "\n",
    "y_pred = model.predict(dev_generator).argmax(axis=1)\n",
    "y_true = dev_generator.classes\n",
    "\n",
    "cmat_df_test=pd.DataFrame(\n",
    "  confusion_matrix(y_true, y_pred, normalize='true').round(2),\n",
    "  index=emotions.values(), \n",
    "  columns=emotions.values()\n",
    "  )\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "heatmap(cmat_df_test,annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion Matrix on Private Test Set')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# configure image data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "# make a prediction using test-time augmentation\n",
    "def tta_prediction(datagen, model, image, n_examples):\n",
    "    # convert image into dataset\n",
    "    samples = np.expand_dims(image, 0)\n",
    "    # prepare iterator\n",
    "    it = datagen.flow(samples, batch_size=n_examples)\n",
    "    # make predictions for each augmented image\n",
    "    yhats = model.predict_generator(it, steps=n_examples, verbose=0)\n",
    "    # sum across predictions\n",
    "    summed = np.sum(yhats, axis=0)\n",
    "    # argmax across classes\n",
    "    return np.argmax(summed)\n",
    " \n",
    " # evaluate a model on a dataset using test-time augmentation\n",
    "def tta_evaluate_model(model, testX, testY):\n",
    "    # configure image data augmentation\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "    # define the number of augmented images to generate per test set image\n",
    "    n_examples_per_image = 7\n",
    "    yhats = list()\n",
    "    for i in range(len(testX)):\n",
    "        # make augmented prediction\n",
    "        yhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n",
    "        # store for evaluation\n",
    "        yhats.append(yhat)\n",
    "    # calculate accuracy\n",
    "    testY_labels = np.argmax(testY, axis=1)\n",
    "    acc = accuracy_score(testY_labels, yhats)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "#TTA_results_test = tta_evaluate_model(model, X_test, Y_test)\n",
    "print('test loss, test acc:', results_test)\n",
    "print('TTA test acc:', TTA_results_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
